{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standalone Notebook for SynFS\n",
    "\n",
    "This notebook is an example notebook with code copied directly rather than imported\n",
    "\n",
    "In this notebook, we demonstrate SynFS on the Syn2 experiment from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chkim/anaconda3/envs/synergy/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "      \"views_dims\": [250, 250],\n",
    "      \"hidden_dims\": [32, 32], \n",
    "      \"s_lam\": 0.1,\n",
    "      \"n_lam\": 1.07,\n",
    "      \"alpha\": 0.25,\n",
    "      \"learning_rate\": 0.001, \n",
    "      \"s_learning_rate\":0.001,\n",
    "      \"batch_size\": 250, \n",
    "      \"weight_decay\": 1e-4,\n",
    "      \"epochs\": 90,\n",
    "      \"threshold\": 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthetic Multi-view Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_views_gt(views):\n",
    "    \"\"\"\n",
    "    Ground truth informative featues used in generating the data \n",
    "    \"\"\"\n",
    "    \n",
    "    gt_views = [np.zeros((view.shape[1])) for view in views]\n",
    "    \n",
    "    # Syn2 GT informative features \n",
    "    gt_views[0][0] = 1\n",
    "    gt_views[0][2] = 1\n",
    "    gt_views[1][1] = 1\n",
    "    gt_views[1][3] = 1\n",
    "    \n",
    "    syn_views = [np.zeros((view.shape[1])) for view in views]\n",
    "    syn_views[0][0] = 1\n",
    "    syn_views[1][1] = 1\n",
    "    return gt_views, syn_views \n",
    "    \n",
    "def generate_multi_dataset(n, dims, seed=0):\n",
    "    \"\"\"\n",
    "    Generate mutli-view dataset \n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    # x generation\n",
    "    views = [np.random.randn(n, dim) for dim in dims]\n",
    "    # y generation rule \n",
    "    y = np.zeros((n, 2))\n",
    "    logit = np.exp(views[0][:,0]*views[1][:,1] + views[0][:,2] + views[1][:,3])\n",
    "    # Compute P(Y=0|X)\n",
    "    prob_0 = np.reshape((logit / (1+logit)), [n, 1])\n",
    "    # Sampling process\n",
    "    y[:, 0] = np.reshape(np.random.binomial(1, prob_0), [n,])\n",
    "    y[:,1] = 1-y[:, 0]\n",
    "    print(\"validate:\",np.unique(y[:,1], return_counts=True))\n",
    "    y = y[:,1]\n",
    "    a_gt, syn_gt = generate_views_gt(views)\n",
    "    return views, y, (a_gt, syn_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate: (array([0., 1.]), array([ 9876, 10124]))\n"
     ]
    }
   ],
   "source": [
    "# Generate data with ground truth\n",
    "views, y, (a_gt, syn_gt) = generate_multi_dataset(20000, [250, 250])\n",
    "\n",
    "data = (views, y)\n",
    "ns_gt = [ai - syn for ai, syn in zip(a_gt, syn_gt)]\n",
    "ns_gt = np.where(np.concatenate(ns_gt))[0]\n",
    "s_gt = np.where(np.concatenate(syn_gt))[0]\n",
    "ground_truth_features = [ns_gt, s_gt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "val_size, test_size, seed = 0.2, 0.2, 0\n",
    "n= len(data[1])\n",
    "\n",
    "X_set = data[0]  # Tuple (v1, v2, ... ,v)\n",
    "y = data[1]\n",
    "\n",
    "# Generate the indices for the train/test \n",
    "# split 64/16/20 train/ val/ test\n",
    "train_indices, test_indices = train_test_split(np.arange(n), test_size=test_size, random_state=seed)\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=val_size, random_state=seed)\n",
    "\n",
    "# Use these indices to split each view\n",
    "tr_X_set = [X[train_indices] for X in X_set]\n",
    "va_X_set = [X[val_indices] for X in X_set]\n",
    "te_X_set = [X[test_indices] for X in X_set]\n",
    "\n",
    "tr_y, va_y, te_y = y[train_indices], y[val_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleDataset(Dataset) :\n",
    "    def __init__(self, data_set, y, device) :\n",
    "        self.data_set = [torch.tensor(data, dtype=torch.float32).to(device) for data in data_set]\n",
    "        self.y = torch.tensor(y).squeeze().long().to(device)\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self) :\n",
    "        return len(self.data_set[0])\n",
    "    \n",
    "    def __getitem__(self, i) :\n",
    "        xs = [data[i] for data in self.data_set]\n",
    "        y = self.y[i]\n",
    "        return (xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SimpleDataset(tr_X_set, tr_y, device=device)\n",
    "trainloader = DataLoader(train_data, batch_size=config['batch_size'], drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpr_fdr(true_groups, predicted_groups):\n",
    "    # True positive rate and false discovery rate.\n",
    "\n",
    "    if len(true_groups) == 0:  # Ground truth not known.\n",
    "        return -1, -1\n",
    "\n",
    "    if len(predicted_groups) == 0 or all(len(sg) == 0 for sg in predicted_groups):\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    predicted_features = np.unique(reduce(np.union1d, predicted_groups))\n",
    "    true_features = np.unique(reduce(np.union1d, true_groups))\n",
    "\n",
    "    overlap = np.intersect1d(predicted_features, true_features).size\n",
    "    tpr = 100 * overlap / len(true_features)\n",
    "    fdr = (\n",
    "        100 * (len(predicted_features) - overlap) / len(predicted_features)\n",
    "    )  # If len(predicted_features) != 0 else 0.0.\n",
    "    return tpr, fdr\n",
    "\n",
    "\n",
    "def strict_jaccard(true_groups, predicted_groups):\n",
    "    # return jacard for true groups (ns, s), predicted_groups\n",
    "    # predicted group will be re-sorted to ns, s (if element==1 -> ns) folloiwng comfps def\n",
    "    if len(true_groups) == 0:  # i.e. we don't know the ground truth.\n",
    "        return -1, len(true_groups), len(predicted_groups)\n",
    "\n",
    "    if len(predicted_groups) == 0 or all(len(sg) == 0 for sg in predicted_groups): # we didn't find anything\n",
    "        return 0, len(true_groups), len(predicted_groups)\n",
    "    \n",
    "    ns_true, s_true = true_groups  # non-synergic, synergic\n",
    "    ns_pred, s_pred = predicted_groups\n",
    "\n",
    "    if len(ns_pred) > 0: \n",
    "        ns_jac = np.intersect1d(ns_true, ns_pred).size / np.union1d(ns_true, ns_pred).size\n",
    "    elif len(ns_pred) == len(ns_true):  # no ground truth \n",
    "        ns_jac = 1\n",
    "    else:\n",
    "        ns_jac = 0\n",
    "        \n",
    "    if len(s_pred) > 0: \n",
    "        s_jac = np.intersect1d(s_true, s_pred).size / np.union1d(s_true, s_pred).size\n",
    "    elif len(s_pred) == len(s_true):  # no ground truth \n",
    "        ns_jac = 1\n",
    "    else:\n",
    "        s_jac =0\n",
    "    return (ns_jac + s_jac) / 2, len(true_groups), len(predicted_groups)\n",
    "\n",
    "def standard_metrics(y_train, y_test, logits, verbose=False):\n",
    "    if len(y_test) != len(logits):\n",
    "        y_test = y_test[:len(logits)]\n",
    "    if isinstance(y_test, np.object_) :\n",
    "        y_test = np.array([y.numpy() for y in y_test])\n",
    "    \n",
    "    def np_softmax(target, all):\n",
    "        softmax = np.exp(target) / np.sum(np.exp(all) ,axis=1)\n",
    "        return softmax\n",
    "        \n",
    "    threshold = np.sum(y_train[y_train==1])/ len(y_train)\n",
    "    threshold = threshold\n",
    "    y_prob = np_softmax(logits[:,1], logits)\n",
    "    y_pred = np.where(y_prob > threshold, 1, 0)\n",
    "    \n",
    "    auroc = roc_auc_score(y_test, logits[:,1])\n",
    "    auprc = average_precision_score(y_test, logits[:,1])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    if verbose:\n",
    "        print(f\"auroc | {auroc.item():.3f}, auprc | {auprc.item():.3f}, accuracy | {accuracy.item():.3f}, f1 | {f1.item():.3f}\")\n",
    "    return auroc, auprc, accuracy, f1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector(nn.Module):\n",
    "    \"\"\"Stochastic gate to discover selected features \n",
    "    \n",
    "    Args:\n",
    "        input_dim : dimension of input features\n",
    "        sigma : constant for gaussian distribution\n",
    "        device : device for the tensors to be created \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, sigma=0.5, mean=0.5) -> None:\n",
    "        super(Selector, self).__init__()\n",
    "        self.mean = mean # 0.5\n",
    "        self.mu = 0.01*torch.randn(input_dim,)\n",
    "        self.mu = torch.nn.Parameter(self.mu, requires_grad=True)\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def forward(self, prev_v, X_mean) -> None: \n",
    "\n",
    "        self.noise = torch.randn(prev_v.size()).to(self.mu.device)\n",
    "        z = self.mu + self.sigma*self.noise.normal_()*self.training # noise normal_ ~N(0,1)\n",
    "        self.z = z  # save z for the same noise\n",
    "        stochastic_gate = self.hard_sigmoid(self.z)\n",
    "        new_v = prev_v*stochastic_gate + X_mean*(1-stochastic_gate)\n",
    "        return new_v\n",
    "    \n",
    "    def hard_sigmoid(self, v):\n",
    "        return torch.clamp(v + self.mean, 0.0, 1.0)\n",
    "    \n",
    "    def regularizer(self, v):\n",
    "        #guassian CDF\n",
    "        return 0.5*(1 + torch.erf(v/math.sqrt(2)))\n",
    "    \n",
    "    def get_gates(self, mode='prob') :\n",
    "        if mode == 'raw':\n",
    "            return self.mu.detach().cpu().numpy()\n",
    "        elif mode == 'prob':\n",
    "            return np.minimum(1.0, np.maximum(0.0, self.mu.detach().cpu().numpy()+self.mean))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim , hidden_dims, output_dim=2, batch_norm=True, dropout=True, activation='relu'):\n",
    "        super(MLP, self).__init__()\n",
    "        modules = self.build_layers(input_dim, hidden_dims, output_dim, batch_norm, dropout, activation)\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        \n",
    "    def base_layer(self, in_features, out_features , batch_norm, dropout, activation):\n",
    "        modules = [nn.Linear(in_features, out_features, bias=True)]\n",
    "        if batch_norm : \n",
    "            modules.append(nn.BatchNorm1d(out_features))\n",
    "        if dropout : \n",
    "            modules.append(nn.Dropout(0.5, True))\n",
    "        if activation :\n",
    "            modules.append(nn.ReLU(True))\n",
    "        layer = nn.Sequential(*modules)\n",
    "        return layer\n",
    "    \n",
    "    def build_layers(self, input_dim, hidden_dims, output_dim, batch_norm, dropout, activation) :\n",
    "        dims = [input_dim]\n",
    "        dims.extend(hidden_dims)\n",
    "        dims.append(output_dim)\n",
    "        nr_hiddens = len(hidden_dims)\n",
    "        modules = []\n",
    "        for i in range(nr_hiddens) :\n",
    "            layer = self.base_layer(dims[i], dims[i+1], batch_norm, dropout, activation)\n",
    "            modules.append(layer)\n",
    "        layer = nn.Linear(dims[-2], dims[-1], bias=True)\n",
    "        modules.append(layer)\n",
    "        return modules\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.reset_parameters()\n",
    "                \n",
    "    def forward(self, input):\n",
    "        out = self.layers(input)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Model, self).__init__()\n",
    "        self.s_selectors = nn.ModuleList()\n",
    "        self.views_dims = config['views_dims']\n",
    "        num_views = len(self.views_dims)\n",
    "        for i in range(num_views):\n",
    "            v_dim = self.views_dims[i]\n",
    "            s_selector = Selector(v_dim)\n",
    "            self.s_selectors.append(s_selector)\n",
    "            setattr(self, f's_selector_{i}', s_selector)\n",
    "\n",
    "        self.shared_predictor = MLP(sum(config['views_dims']), config['hidden_dims'],\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _standard_truncnorm_sample(lower_bound, upper_bound, sample_shape=torch.Size()):\n",
    "    r\"\"\"\n",
    "    Weight initialization usggested in STG\n",
    "    \"\"\"\n",
    "    x = torch.randn(sample_shape)\n",
    "    done = torch.zeros(sample_shape).byte()\n",
    "    while not done.all():\n",
    "        proposed_x = lower_bound + torch.rand(sample_shape) * (upper_bound - lower_bound)\n",
    "        if (upper_bound * lower_bound).lt(0.0):  # of opposite sign\n",
    "            log_prob_accept = -0.5 * proposed_x**2\n",
    "        elif upper_bound < 0.0:  # both negative\n",
    "            log_prob_accept = 0.5 * (upper_bound**2 - proposed_x**2)\n",
    "        else:  # both positive\n",
    "            assert(lower_bound.gt(0.0))\n",
    "            log_prob_accept = 0.5 * (lower_bound**2 - proposed_x**2)\n",
    "        prob_accept = torch.exp(log_prob_accept).clamp_(0.0, 1.0) #inplace\n",
    "        accept = torch.bernoulli(prob_accept).byte() & ~done # return the prob_accept shape matrix where done is 0 \n",
    "        if accept.any():\n",
    "            accept = accept.bool()\n",
    "            x[accept] = proposed_x[accept]\n",
    "            accept = accept.byte()\n",
    "            done |= accept # |=, in-place bitwise OR operator to done \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SynFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynFS(object):\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        construct \n",
    "        s_model : a set of synergistic selector + predictor\n",
    "        n_model : a set of non-synergistic selecotr + predictor\n",
    "        inf_model : predictor\n",
    "        \"\"\"\n",
    "        \n",
    "        self.config = config\n",
    "        self.batch_size = self.config['batch_size']\n",
    "        self.device = self.get_device()\n",
    "        self.loss = nn.CrossEntropyLoss(reduction='none')\n",
    "        self.cos_loss = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "        self.s_model = Model(config)\n",
    "        self.s_model.apply(self.init_weights)\n",
    "        self.s_model.to(self.device)\n",
    "\n",
    "        self.n_model = Model(config)\n",
    "        self.n_model.apply(self.init_weights)\n",
    "        self.n_model.to(self.device)\n",
    "        \n",
    "        self.inf_model = MLP(sum(config['views_dims']), config['hidden_dims'])\n",
    "        self.inf_model.apply(self.init_weights)\n",
    "        self.inf_model.to(self.device)\n",
    "        \n",
    "        self.sp_params = list(self.s_model.shared_predictor.parameters()) \n",
    "        self.np_params = list(self.n_model.shared_predictor.parameters()) \n",
    "\n",
    "        self.s_params = [p for selector in self.s_model.s_selectors for p in selector.parameters()] \n",
    "        self.n_params = [p for selector in self.n_model.s_selectors for p in selector.parameters()] \n",
    "        \n",
    "        self.p_opt = torch.optim.Adam(self.sp_params+self.np_params, \n",
    "                                      lr=config['learning_rate'], \n",
    "                                      weight_decay=config['weight_decay']) # \n",
    "        self.synergy_opt = torch.optim.Adam(self.s_params, lr=config['s_learning_rate'])# list of optimizers\n",
    "        self.nsynergy_opt = torch.optim.Adam(self.n_params, lr=config['s_learning_rate'])\n",
    "        self.inf_opt = torch.optim.Adam(self.s_params + self.n_params + list(self.inf_model.parameters()),\n",
    "                                            lr=config['learning_rate'], \n",
    "                                            weight_decay=config['weight_decay'])\n",
    "    def get_device(self):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        return device\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            stddev = torch.tensor(0.1)\n",
    "            shape = m.weight.shape\n",
    "            m.weight = nn.Parameter(_standard_truncnorm_sample(lower_bound = -2*stddev, upper_bound = 2 * stddev, sample_shape = shape))\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def get_reg(self, selector):\n",
    "        reg = selector.regularizer\n",
    "        #reg = reg((selector.mu + selector.mean) / selector.sigma) #cdf\n",
    "        reg = torch.mean(reg(selector.mu / selector.sigma))\n",
    "        return reg\n",
    "    \n",
    "    def get_gates(self, model):\n",
    "        S = [selector.hard_sigmoid(selector.mu.detach()) for selector in model.s_selectors]\n",
    "        return S\n",
    "\n",
    "    def mask_generator(self, batch_size=None):\n",
    "        \"\"\"\n",
    "        Generate mask to use shared predictor with V marginal selectors\n",
    "        \"\"\"\n",
    "        #generating mask for latent space\n",
    "        # return a list of masks for each view \n",
    "        if batch_size is None:\n",
    "            batch_size = self.config['batch_size']\n",
    "        masks = []\n",
    "        view_sum = sum(self.config['views_dims'])\n",
    "        cumsum = np.cumsum(self.config['views_dims'])\n",
    "        blank_mask = torch.zeros(batch_size, view_sum, device=self.device)\n",
    "        for v in range(len(self.config['views_dims'])):\n",
    "            v_mask = blank_mask.clone()\n",
    "            if v == 0 :\n",
    "                v_mask[:, :cumsum[v]] = 1\n",
    "            else:\n",
    "                v_mask[:, cumsum[v-1]:cumsum[v]] = 1\n",
    "            masks.append(v_mask)\n",
    "        return masks\n",
    "    \n",
    "    def simple_forward(self, model, S, views, X_mean_set):\n",
    "        \"\"\"\n",
    "        simple forward to get P(Y|X_g_s) or P(Y|X_g_n)\n",
    "        \"\"\"\n",
    "        masks = self.mask_generator(batch_size=views[0].shape[0])\n",
    "        s_z = [S[i]*views[i]+(1-S[i])*X_mean_set[i] for i in range(len(views))]\n",
    "        logits = []\n",
    "        for v in range(len(self.config['views_dims'])):\n",
    "            logit = model.shared_predictor(torch.cat(s_z, dim=1)*masks[v])\n",
    "            logits.append(logit)\n",
    "        bar_logit = model.shared_predictor(torch.cat(s_z, dim=1))\n",
    "        logits.append(bar_logit)\n",
    "        return logits \n",
    "\n",
    "    def train_step(self, data, X_mean_set):\n",
    "        \n",
    "        \"\"\"\"\n",
    "        Train Predictor -> synergistic selector -> non-syergistic selector\n",
    "        only corresponding weights specified for the loss are updated \n",
    "        \n",
    "        \"\"\"\n",
    "        self.s_model.train()\n",
    "        self.s_model.train()\n",
    "        self.inf_model.train()\n",
    "        \n",
    "        views, y = data[:-1][0], data[-1]\n",
    "        masks = self.mask_generator()\n",
    "        \n",
    "        # new v \n",
    "        v_s = [selector(views[i], X_mean_set[i]) for i, selector in enumerate(self.s_model.s_selectors)] \n",
    "        v_n = [selector(views[i], X_mean_set[i]) for i, selector in enumerate(self.n_model.s_selectors)]\n",
    "        z_s = [selector.z for selector in self.s_model.s_selectors]\n",
    "        z_n = [selector.z for selector in self.n_model.s_selectors]\n",
    "        \n",
    "        ###############################\n",
    "        ###        Predictor       ###\n",
    "        ###############################\n",
    "        # with the same noise as with selectors  \n",
    "        #  non-synergic\n",
    "        \n",
    "        n_logits, s_logits = [], []\n",
    "        for v in range(len(self.config['views_dims'])):\n",
    "            logit = self.n_model.shared_predictor(torch.cat(v_n, dim=1)*masks[v])\n",
    "            n_logits.append(logit)\n",
    "        n_v_bar_logits = self.n_model.shared_predictor(torch.cat(v_n, dim=1))\n",
    "        n_logits.append(n_v_bar_logits)\n",
    "        #  synergic\n",
    "        for v in range(len(self.config['views_dims'])):\n",
    "            logit = self.s_model.shared_predictor(torch.cat(v_s, dim=1)*masks[v])\n",
    "            s_logits.append(logit)\n",
    "        s_v_bar_logits = self.s_model.shared_predictor(torch.cat(v_s, dim=1))\n",
    "        s_logits.append(s_v_bar_logits)\n",
    "        \n",
    "        n_losses = [self.loss(logit, y) for logit in n_logits]\n",
    "        s_losses = [self.loss(logit, y) for logit in s_logits]\n",
    "        p_loss = torch.mean(torch.sum(torch.stack(n_losses+s_losses, dim=1), dim=1))\n",
    "\n",
    "        self.p_opt.zero_grad() \n",
    "        p_loss.backward(retain_graph=True) # s_selector, n_selector, predictors \n",
    "        self.p_opt.step()  # update predictors\n",
    "        \n",
    "        #####################################\n",
    "        ###   Synergic Selector and Inf   ###\n",
    "        #####################################\n",
    "        \n",
    "        n_logits, s_logits = [], []\n",
    "        # informative gate (max(synergic gate , non-synergic gate))\n",
    "        all_gates = [self.s_model.s_selectors[0].hard_sigmoid(torch.max(s, n)) for s, n in zip(z_s, z_n)]  # with noise\n",
    "        all_v = [views[i]*gate + X_mean_set[i]*(1-gate) for i, gate in enumerate(all_gates)]\n",
    "        gate_s, gate_n = [self.s_model.s_selectors[0].hard_sigmoid(s) for s in z_s], [self.s_model.s_selectors[0].hard_sigmoid(n) for n in z_n] \n",
    "        s_regs =[self.get_reg(selector) for selector in self.s_model.s_selectors]\n",
    "        \n",
    "        all_bar_logits = self.inf_model(torch.cat(all_v, dim=1))\n",
    "        all_bar_loss = self.loss(all_bar_logits, y)\n",
    "\n",
    "        for v in range(len(self.config['views_dims'])):\n",
    "            logit = self.s_model.shared_predictor(torch.cat(v_s, dim=1)*masks[v])\n",
    "            s_logits.append(logit)\n",
    "        s_v_bar_logits = self.s_model.shared_predictor(torch.cat(v_s, dim=1))\n",
    "\n",
    "        s_losses = [self.loss(logit, y) for logit in s_logits]\n",
    "        s_v_bar_loss = self.loss(s_v_bar_logits, y)\n",
    "        inf_loss = torch.mean(all_bar_loss) \n",
    "        \n",
    "        synergy_loss = torch.mean(s_v_bar_loss - torch.sum(torch.stack(s_losses, dim=1), dim=1) \\\n",
    "                                + self.config['s_lam']*torch.mean(torch.stack(s_regs)) \\\n",
    "                                )\n",
    "\n",
    "        self.inf_opt.zero_grad()\n",
    "        self.synergy_opt.zero_grad()\n",
    "        inf_loss.backward(retain_graph=True) #s selecotr, n selector, ai predictor \n",
    "        synergy_loss.backward() # s selector, predictors \n",
    "\n",
    "        self.inf_opt.step()  \n",
    "        self.synergy_opt.step()\n",
    "        \n",
    "        ##########################\n",
    "        ###   Non-syneristic   ###\n",
    "        ##########################\n",
    "\n",
    "        sim = torch.nn.functional.cosine_similarity(torch.cat(gate_s, dim=1), torch.cat(gate_n, dim=1), dim=1) #(batch, alldim)\n",
    "        ns_regs =[self.get_reg(selector) for selector in self.n_model.s_selectors]\n",
    "        \n",
    "        for v in range(len(self.config['views_dims'])):\n",
    "            logit = self.n_model.shared_predictor(torch.cat(v_n, dim=1)*masks[v])\n",
    "            n_logits.append(logit)\n",
    "        n_v_bar_logits = self.n_model.shared_predictor(torch.cat(v_n, dim=1))\n",
    "        \n",
    "        n_losses = [self.loss(logit, y) for logit in n_logits]\n",
    "        n_v_bar_loss = self.loss(n_v_bar_logits, y)\n",
    "\n",
    "        nsynergy_loss = torch.mean(-n_v_bar_loss + torch.sum(torch.stack(n_losses, dim=1), dim=1) \n",
    "                                   + self.config['n_lam']*torch.mean(torch.stack(ns_regs)) # same mu for all batch\n",
    "                                   + self.config['alpha']*sim \n",
    "                                   )\n",
    "        \n",
    "        self.nsynergy_opt.zero_grad()\n",
    "        nsynergy_loss.backward() # n selector, predcitor \n",
    "        self.nsynergy_opt.step()\n",
    "        \n",
    "        return synergy_loss, nsynergy_loss, inf_loss\n",
    "\n",
    "    def predict(self, views, te_X_set):\n",
    "        self.s_model.eval()\n",
    "        self.n_model.eval()\n",
    "        self.inf_model.eval()\n",
    "        \n",
    "        X_mean_set = [torch.mean(torch.Tensor(X).to(self.device), dim=0) for X in te_X_set] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            S = self.get_gates(self.s_model)\n",
    "            NS = self.get_gates(self.s_model)\n",
    "            all_mu = [torch.max(s, ns) for s, ns in zip(S, NS)]\n",
    "            all_z = [all_mu[i]*views[i]+(1-all_mu[i])*X_mean_set[i] for i in range(len(views))]\n",
    "            all_bar_logits = self.inf_model(torch.cat(all_z, dim=1))\n",
    "        return all_bar_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====epoch 0 syn_loss -0.693=====\n",
      "predicted synergistic features\n",
      "[]\n",
      "predicted non-synergistic features\n",
      "[]\n",
      "=====epoch 30 syn_loss -0.665=====\n",
      "predicted synergistic features\n",
      "[  0 251]\n",
      "predicted non-synergistic features\n",
      "[  2 253]\n",
      "=====epoch 60 syn_loss -0.681=====\n",
      "predicted synergistic features\n",
      "[  0 251]\n",
      "predicted non-synergistic features\n",
      "[  2 253]\n"
     ]
    }
   ],
   "source": [
    "synfs = SynFS(config)\n",
    "X_mean_set = [torch.mean(torch.Tensor(X).to(device), dim=0) for X in tr_X_set]\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    syn_loss = 0\n",
    "    for batch in trainloader:\n",
    "        synergy_loss, nsynergy_loss, inf_loss = synfs.train_step(batch, X_mean_set)\n",
    "        syn_loss +=synergy_loss\n",
    "    avg_syn_loss = syn_loss/len(trainloader)\n",
    "    if epoch % 30 == 0:\n",
    "        print(f'=====epoch {epoch} syn_loss {avg_syn_loss.item():.3f}=====')\n",
    "        print('predicted synergistic features')\n",
    "        print(np.where(np.concatenate([g.cpu().numpy() for g in synfs.get_gates(synfs.s_model)])>0.7)[0])\n",
    "        print('predicted non-synergistic features')\n",
    "        print(np.where(np.concatenate([g.cpu().numpy() for g in synfs.get_gates(synfs.n_model)])>0.7)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth_syn | [  2 253] ,ground_truth_non-syn |  [  0 251]\n",
      "predicted_syn | [  2 253] ,predicted_non-syn |  [  0 251]\n",
      "Jaccard Index: 1.000, True Positive Rate: 100.000%, False Discovery Rate: 0.000%\n"
     ]
    }
   ],
   "source": [
    "# Get group similarity and group structure.\n",
    "s = [gate.cpu().numpy() for gate in synfs.get_gates(synfs.s_model)]\n",
    "n = [gate.cpu().numpy() for gate in synfs.get_gates(synfs.n_model)]\n",
    "n_predicted = np.where(np.concatenate(n)>0.7)[0]\n",
    "s_predicted = np.where(np.concatenate(s)>0.7)[0]\n",
    "predicted_features = [n_predicted, s_predicted]\n",
    "\n",
    "# Get group similarity and group structure.\n",
    "tpr, fdr = tpr_fdr(ground_truth_features, predicted_features)\n",
    "j_index, ntrue, npredicted = strict_jaccard(ground_truth_features, predicted_features)\n",
    "\n",
    "print(\"ground_truth_syn |\", ground_truth_features[0], \",ground_truth_non-syn | \", ground_truth_features[1])\n",
    "print(\"predicted_syn |\", predicted_features[0], \",predicted_non-syn | \", predicted_features[1] )\n",
    "print(\n",
    "    \"Jaccard Index: {:.3f}, True Positive Rate: {:.3f}%, False Discovery Rate: {:.3f}%\".format(\n",
    "        j_index, tpr, fdr\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auroc | 0.655, auprc | 0.660, accuracy | 0.604, f1 | 0.652\n"
     ]
    }
   ],
   "source": [
    "data = SimpleDataset(te_X_set, te_y, device=device)\n",
    "testloader = DataLoader(data, batch_size=len(te_y))\n",
    "           \n",
    "res = []\n",
    "for x, y in testloader:\n",
    "    logits = synfs.predict(x, X_set)\n",
    "    res.append(logits.detach().cpu().numpy())\n",
    "logits = np.concatenate(res)\n",
    "\n",
    "auroc, auprc, accuracy, f1 = standard_metrics(tr_y, te_y, logits, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synergy",
   "language": "python",
   "name": "synergy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
